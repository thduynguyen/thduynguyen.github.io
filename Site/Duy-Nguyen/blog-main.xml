<?xml version="1.0"?>
<rss xmlns:iweb="http://www.apple.com/iweb" version="2.0">
  <channel>
    <title>Duy-Nguyen</title>
    <link>http://www.cc.gatech.edu/~dnth/Site/Duy-Nguyen</link>
    <description>I am a PhD student at IRIM, Georgia Tech, working with Prof. Frank Dellaert on perception and optimal control problems for autonomous navigation on quadrotors.
Before falling in love with robotics, I have been playing with monocular tracking for AR on mobile phones in Prof. Blair MacIntyre&#x2019;s lab and Nokia Research Centers. I received an M.Eng. degree from NUS for some work in AR.
Please see here for the full list of my work.


If you find my Vietnamese name is hard to pronounce, here is a hint: Duy-Nguyen.</description>
    <item>
      <title>Model Predictive Control</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2014/2/17_Model_Predictive_Control.html</link>
      <guid isPermaLink="false">f86b1972-7e4c-4182-a736-3fd1c4dd7b5d</guid>
      <pubDate>Mon, 17 Feb 2014 00:48:32 -0500</pubDate>
      <description>I have been interested in optimal control during the past two years. Together with Prof. &lt;a href="http://folio.jhu.edu/faculty/Marin_Kobilarov"&gt;Marin Kobilarov&lt;/a&gt; at JHU, and my advisor, Prof. &lt;a href="http://www.cc.gatech.edu/~dellaert"&gt;Frank Dellaert&lt;/a&gt;, I have a paper at &lt;a href="http://www.uasconferences.com/"&gt;ICUAS&lt;/a&gt; on using factor graphs as a unified framework for both estimation and control problems. We derived constrained factors to discretize the system&#x2019;s dynamics/kinematics by doing integration on Lie-group manifolds. We also derived SQP on factor graphs and discovered an interesting connection between the primal and dual factor graphs. More details in our &lt;a href="Entries/2014/2/17_Model_Predictive_Control_files/Ta14icuas_final.pdf"&gt;ICUAS paper&lt;/a&gt;.</description>
      <iweb:image href="Media/object027.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2014/2/17_Model_Predictive_Control.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Linear-time SLAM with Incremental Tree Filtering</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2014/2/6_Linear-time_SLAM_with_Incremental_Tree_Filtering.html</link>
      <guid isPermaLink="false">5b81bb2b-5d8c-46ed-887d-7f108fd94d7a</guid>
      <pubDate>Thu, 6 Feb 2014 00:06:48 -0500</pubDate>
      <description>For my thesis work, I explored object-centric based approaches for fast object estimation and robot localization. My exploration led to linear-time approximation algorithms for information filtering-based SLAM, which is traditionally cubic with respect to the number of features of the object. More details in my &lt;a href="Entries/2014/2/6_Linear-time_SLAM_with_Incremental_Tree_Filtering_files/Ta14iros_finalrev2-1.pdf"&gt;IROS paper&lt;/a&gt;.</description>
      <iweb:image href="Media/object028.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2014/2/6_Linear-time_SLAM_with_Incremental_Tree_Filtering.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Monocular Parallel Tracking and Mapping with Odometry Fusion</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2013/10/8_Parallel_Tracking_and_Mapping_for_Fusing_Monocular_with_Odometry.html</link>
      <guid isPermaLink="false">190f4035-e05b-4a66-ba67-d862b034fc2f</guid>
      <pubDate>Tue, 8 Oct 2013 06:08:46 -0400</pubDate>
      <description>UPDATE 2014-04: Our &lt;a href="http://www.sciencedirect.com/science/article/pii/S0921889014000542"&gt;RAS journal paper&lt;/a&gt; combining this Parallel framework, &lt;a href="Entries/2012/3/30_Entry_1.html"&gt;Wall-Floor Features&lt;/a&gt; and &lt;a href="Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html"&gt;Vistas&lt;/a&gt; for autonomous indoor navigation has been accepted for publication!&lt;br/&gt;&lt;br/&gt;This is my attempt to improve autonomous indoor flight. Our current framework with &lt;a href="Entries/2012/3/30_Entry_1.html"&gt;Wall-Floor Features&lt;/a&gt; and &lt;a href="Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html"&gt;Vistas&lt;/a&gt; is too &lt;br/&gt;slow for real-time control, so we try to improve its speed with a parallel tracking and mapping framework similar to &lt;a href="http://www.robots.ox.ac.uk/~gk/PTAM/"&gt;PTAM&lt;/a&gt;. The fast tracking thread maintains real-time localization while the slow mapping thread allows high quality wall structure inference. &lt;br/&gt;&lt;br/&gt;Unfortunately, although &lt;a href="http://www.robots.ox.ac.uk/~gk/PTAM/"&gt;PTAM&lt;/a&gt; and other pure feature-based monocular SLAM systems are great, they don&#x2019;t work for robots with a single frontal camera in typical indoor environments. The first reason for their failure is that there are not enough corner-type features in the scene, while PTAM typically needs hundreds features to work robustly. The second reason is more important: There are not enough motion parallax to build and maintain a good map, due to the typical robot motions. Since the robot typically moves forward and most features are in front of it, we cannot get reliable depth information. When the robot yaws away from a known map towards unknown regions, PTAM will also break down since it cannot triangulate new landmarks for the map and hence not being able to localize itself.&lt;br/&gt;&lt;br/&gt;Thus, we have to fuse monocular PTAM with odometry measurements to cope with the lack of features, and maintain robustness during pure camera rotations. Unfortunately, &lt;a href="http://publications.asl.ethz.ch/files/achtelik11onboard.pdf"&gt;other&lt;/a&gt; PTAM-odometry &lt;a href="http://publications.asl.ethz.ch/files/weiss11real.pdf"&gt;fusion&lt;/a&gt; methods treats PTAM as a black box, so they cannot prevent PTAM&#x2019;s breakdown in these cases. &lt;br/&gt;&lt;br/&gt;Using factor graphs, we developed a new parallel tracking and mapping framework that is suitable for robot navigation by fusing visual data with odometry measurements in a principled manner. More details can be found in &lt;a href="http://borg.cc.gatech.edu/papers/335"&gt;our workshop paper&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;With this framework, we can finally enable the ARDrone to fly and turn autonomously. This is the first autonomous turn with our parallel framework:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;And here is the complete mission towards the end of the hallway. The video stops a bit because our Wifi connection was lost right after the drone turned. I was sitting quite far away and had to run after it to see what happened.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object030.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2013/10/8_Parallel_Tracking_and_Mapping_for_Fusing_Monocular_with_Odometry.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Wall-Floor Features for Lateral Collision Avoidance</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2012/3/30_Entry_1.html</link>
      <guid isPermaLink="false">86d706f7-ee66-4673-8d24-51c912e59022</guid>
      <pubDate>Fri, 30 Mar 2012 05:33:51 -0400</pubDate>
      <description>This is another attempt to enable indoor autonomous flight on an ARDrone. I worked with &lt;a href="http://www.korobotics.com/"&gt;Kyel Ok&lt;/a&gt; in this project. Kyel, now a student at MIT, is a very good friend of mine with a great desire and talent to make things work.&lt;br/&gt;&lt;br/&gt;&lt;a href="Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html"&gt;Vistas&lt;/a&gt; are good for straight flight, but we need the wall information to avoid lateral collisions. Due to the &lt;a href="Entries/2011/11/10_MonoSLAM_in_feature-poor_environments.html"&gt;limitations of vertical edges&lt;/a&gt;, I tried to make use of more salient information in the images. The wall-floor intersection lines are great candidates, so I look for intersections between vertical edges and the wall-floor intersection lines. These features encode the wall structure and are easy to detect using steerable filters as shown in the video below. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Here is one of our offline results for mapping those features:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Here is an online SLAM version:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is our first autonomous turn ever with &lt;a href="Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html"&gt;vistas&lt;/a&gt; and WFFs:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object031.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2012/3/30_Entry_1.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Vistas for Rotation Aiding and Autonomous Straight Flight</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html</link>
      <guid isPermaLink="false">2af608b8-a5a1-458c-aec3-60c574853b76</guid>
      <pubDate>Tue, 22 Nov 2011 00:42:15 -0500</pubDate>
      <description>&lt;a href="http://borg.cc.gatech.edu/people/cbeall3"&gt;Chris&lt;/a&gt; and I, together with &lt;a href="http://www.korobotics.com/"&gt;Kyel&lt;/a&gt;, have realized Frank&#x2019;s idea to use Vistas, our name for far-away features, for aiding rotation estimation on the drone. &lt;br/&gt;As &lt;a href="https://dl.dropboxusercontent.com/u/2073820/site/Site/Media/vista_noise2.mov"&gt;shown below&lt;/a&gt;, Vistas are detected from tracking SURF features and observe their scale changes. See our &lt;a href="http://borg.cc.gatech.edu/papers/beall12fusion"&gt;Fusion paper&lt;/a&gt; here for more details. (Also, see my &lt;a href="Entries/2008/12/5_SURFTrac.html"&gt;related work&lt;/a&gt; at CVPR 2009)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Since they are far-away, we also used Vistas as directions for autonomous flight and exploration. The following &lt;a href="https://dl.dropboxusercontent.com/u/2073820/site/Site/Media/AutoFlight1.m4v"&gt;video&lt;/a&gt; shows our first autonomous flight with vistas. It successfully passed through the narrow door at the end of the hallway!&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;And &lt;a href="https://dl.dropboxusercontent.com/u/2073820/site/Site/Media/AutoFlight2.m4v"&gt;this&lt;/a&gt; is our full autonomous flight towards vistas with human-assisted turns at the hallway corners. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;You can see the drone collides to the walls. We later tried to avoid lateral collisions with &lt;a href="Entries/2012/3/30_Entry_1.html"&gt;Wall-Floor Features&lt;/a&gt;. More details can be found in our &lt;a href="http://borg.cc.gatech.edu/papers/ok12vicomor"&gt;workshop paper&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;Update 2014-04: Scherer&#x2019;s group at CMU recently used a very &lt;a href="http://www.ri.cmu.edu/publication_view.html?pub_id=7452"&gt;similar&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=9QQCGLKuUxc"&gt;idea&lt;/a&gt; to enable obstacle avoidance on an ARDrone.</description>
      <iweb:image href="Media/object029.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>MonoSLAM with Vertical Edges</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/11/10_MonoSLAM_in_feature-poor_environments.html</link>
      <guid isPermaLink="false">dd3c807d-7149-41d3-9cad-54c153dfc7b3</guid>
      <pubDate>Thu, 10 Nov 2011 00:29:08 -0500</pubDate>
      <description>I considered feature-based monocular SLAM more or less solved due to lots of great results like Andrew Davison&#x2019;s &lt;a href="http://www.doc.ic.ac.uk/~ajd/"&gt;MonoSLAM&lt;/a&gt; and Georg Klein&#x2019;s &lt;a href="http://www.robots.ox.ac.uk/~gk/PTAM/"&gt;PTAM&lt;/a&gt; systems. So I challenged myself to push forward monoSLAM in other common but challenging environments like the hallway above with a little number of corner-type features.&lt;br/&gt;&lt;br/&gt;My first attempt is to do SLAM with vertical edges which are one of the most salient features in these types of environments. I used EM for data association, and used inverse depth parameterization to ease the initialization problem. The results are shown in the following video.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;There are two major problems I have encountered in this work. First, the prior for the inverse depth parameterization is very hard to tune. It is not a normal Gaussian distribution, since the inverse depth needs to be larger than zero. I used a Gamma distribution as its prior and a small change in its parameters might affect the results significantly. Maybe some learning might help to alleviate the problem. Second, my final goal is to enable perception for autonomous navigation, but sparse vertical edges do not give me enough information to infer wall structures to avoid them. So I switched to play with &lt;a href="Entries/2011/11/22_Vistas_for_Rotation_Aiding_and_Autonomous_Flight.html"&gt;other types of features&lt;/a&gt;.&lt;br/&gt;</description>
      <iweb:image href="Media/object032.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/11/10_MonoSLAM_in_feature-poor_environments.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Flying through a Forest</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/4/21_Flying_through_a_Forest.html</link>
      <guid isPermaLink="false">5b68acc8-694d-4e60-bc2f-ce6d4b74839b</guid>
      <pubDate>Thu, 21 Apr 2011 00:14:47 -0400</pubDate>
      <description>In this project, we tried to enable enough perception for an ARDrone quadrotor to support its autonomous flight through a forest. The specific goal is to make it aware of local tree trunks to avoid them. This is a very difficult problem, and we later knew that there is another separate &lt;a href="http://www.onr.navy.mil/Media-Center/Press-Releases/2012/ONR-Funded-Research-Takes-Flight.aspx"&gt;MURI project&lt;/a&gt; fully dedicated to this problem, involving many famous researchers from top universities. &lt;br/&gt;&lt;br/&gt;The perception problem is very challenging due to the limited sensing and computational capabilities of the ARDrone, which only has a monocular frontal camera to observe trees. Furthermore, state-of-the-art computer vision methods have not been able to operate in these kinds of wild forested environments with lots of occlusions and ambiguity. &lt;br/&gt;&lt;br/&gt;We tried to realize Frank&#x2019;s vision for a bio-inspired two-system perception framework, which includes a peripheral system to pick out motion-salient objects, which are near by, and a foveal system to track them over time. The peripheral system is based on Richard Roberts&#x2019;s &lt;a href="http://borg.cc.gatech.edu/papers/roberts09cvpr"&gt;optical-flow subspace&lt;/a&gt; method. For the foveal system, we used a model-based edge tracking method.&lt;br/&gt;&lt;br/&gt;The video below shows our results. Please refer to our &lt;a href="http://frank.dellaert.com/pubs/Roberts12spie.pdf"&gt;SPIE paper&lt;/a&gt; for more details. We are able to detect and track trees robustly in a white sky background. However, we have not been able to handle occlusions and tree ambiguity reliably. Another crucial problem is the optical-flow-based peripheral system for motion saliency detection cannot detect objects in the moving direction due to the lack of motion parallax at the Focus of Expansion point.&lt;br/&gt;&lt;br/&gt;I was lucky enough to have a chance to work with &lt;a href="http://borg.cc.gatech.edu/people/gtg516a/"&gt;Richard&lt;/a&gt; and &lt;a href="http://people.csail.mit.edu/jstraub/"&gt;Julian Straub&lt;/a&gt; on this project. Richard is one of the most talented people and best programmers I have known in my life. I&#x2019;ve learned a lot from him through out the years. He is now with Google X. Julian is a very cool guy. He was an intern in our lab, and is now doing his PhD at MIT.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object005.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2011/4/21_Flying_through_a_Forest.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Template-based AR Tracking</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/8/20_Template-based_Tracking_for_AR.html</link>
      <guid isPermaLink="false">927e4716-4b12-447e-b1d5-9f1d29b11396</guid>
      <pubDate>Thu, 20 Aug 2009 23:15:46 -0400</pubDate>
      <description>Inspired by Georg Klein&#x2019;s &lt;a href="http://www.robots.ox.ac.uk/~gk/PTAM/"&gt;PTAM&lt;/a&gt; and Daniel Wagner et. al&#x2019;s &lt;a href="http://tinyurl.com/mx6fby5"&gt;paper&lt;/a&gt;, I developed a similar template-based tracker for AR during my internship at Nokia. &lt;a href="https://research.nvidia.com/users/yun-ta-tsai"&gt;Yun-Ta Tsai&lt;/a&gt;, an awesome graphics programmer, has ported and integrated it to his awesome AR framework for N900 phones. &#x2028;Unfortunately, Nokia Hollywood lab was closed and I guess nobody uses it anymore. However, I can guarantee that the tracker performance was comparable to &lt;a href="http://handheldar.icg.tugraz.at/media/RobustHighSpeedTracking_PC_v2.avi"&gt;this awesome tracker&lt;/a&gt;, which has been grown up to the famous Qualcomm&#x2019;s &lt;a href="https://www.vuforia.com/"&gt;Vuforia framework&lt;/a&gt; nowadays.</description>
      <iweb:image href="Media/object021.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/8/20_Template-based_Tracking_for_AR.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>SURFTrac3D</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/8/15_SURFTrac3D.html</link>
      <guid isPermaLink="false">f9f95a78-3ef8-49f9-b8fd-46225a171fa6</guid>
      <pubDate>Sat, 15 Aug 2009 15:07:58 -0400</pubDate>
      <description>This is the result of my second summer internship with Nokia at NRC Hollywood lab. I experimented applying &lt;a href="Entries/2008/12/5_SURFTrac.html"&gt;SURFTrac&lt;/a&gt; for 3D outdoor Augmented Reality. &lt;br/&gt;&lt;br/&gt;I first built a database of SURF features extracted  from a set of images and used &lt;a href="http://phototour.cs.washington.edu/bundler/"&gt;Bundler&lt;/a&gt; to construct their 3D coordinates. Then, in the first image captured from the video stream, I extracted SURF features and matched with those in the database. After that, I use traditional RANSAC with Homography model to reject wrong matches. The correct matches are then used to initialize the camera pose.&lt;br/&gt;&lt;br/&gt;SURFTrac is used to track SURF features in subsequent camera frames. New features in the frames are detected and matched with the database. From those matches, I compute 6 dof camera poses for each frame and augment all the 3D points for visualization. All the red dots in the video below are those 3D points in the database augmented onto the real scene. They are NOT 2D detected features. Please note how stable their projections are.&lt;br/&gt;&lt;br/&gt;I also render a panel of Nokia logo onto the building to show that it&#x2019;s a real 3D scene tracking. You might notice that the panel is not properly aligned. However, the main error doesn&#x2019;t come from wrong pose estimation, but rather from my bad modeling skill. The position of the plane panel in 3D virtual world is not properly setup. You can see how &#x201C;stable&#x201D; the misalignment is. :-)&lt;br/&gt;&lt;br/&gt;&lt;a href="http://www.youtube.com/watch?v=Z1CZ5DoAiSU"&gt;Video&lt;/a&gt;:&lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object008.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/8/15_SURFTrac3D.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Art of Defense</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/3/11_Art_of_Defense.html</link>
      <guid isPermaLink="false">b4ac06aa-384c-4614-8adf-418da594b404</guid>
      <pubDate>Thu, 12 Mar 2009 01:01:31 -0400</pubDate>
      <description>Art of Defense explores novel game design and interaction techniques for mobile AR on commodity phone (Nokia N95). The design leverages the phone&#x2019;s screen size limitations as part of the game challenge. Its dynamic multi-marker building technique enables the game&#x2019;s mobility and portability, whereas the sketch-based interaction enhances the tangibility and content relationship between the physical and virtual worlds. These techniques encourage both physical movement and tangible interaction with the game pieces -- the two key differentiators between AR and other phone-based games.&#xA0; Art of Defense also demonstrates a new type of AR exploration game, where the players control which parts of the space they can explore. Our first version is shown at ISMAR&#x2019;08. &lt;br/&gt;&lt;br/&gt;Our second version is a 2-player collaborative game. We redesigned the game significantly to enforce collaboration between players. We replaced sketches with tokens to simplify the interactions and make it really tangible. The game was also fine-tuned carefully to become really interesting. Our user study reveals many interesting lessons about table-top AR collaborative games. For more details about the game, please see our paper below in Sandbox SIGGRAPH 2009.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Paper: &lt;br/&gt;&lt;br/&gt;D.N. Ta, K. Raveendran, Y. Xu, K. Spreen, and B. MacIntyre, &#x201C;Art of defense: a collaborative handheld augmented reality board game,&#x201D; Proceedings of the 2009 ACM SIGGRAPH Symposium on Video Games, 2009, pp. 135&#x2013;142. (&lt;a href="http://www.cc.gatech.edu/~dnth/Site/Publications_files/AoD-Sandbox-Final.pdf"&gt;pdf&lt;/a&gt;)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;a href="http://www.youtube.com/watch?v=sR_plHtkZ-Q"&gt;Video version 1&lt;/a&gt;: single player game with sketches:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;a href="http://www.youtube.com/watch?v=hSUuvgklsZw"&gt;Video version 2&lt;/a&gt;: multiplayer collaborative AR game with tangible tokens:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Update: An interesting &lt;a href="http://vimeo.com/1766417"&gt;video&lt;/a&gt; of our very first version on vimeo, captured by Dylski (?). I believe this was my demo in ISMAR&#x2019;08 at Cambridge, UK. </description>
      <iweb:image href="Media/object023.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2009/3/11_Art_of_Defense.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>SURFTrac</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2008/12/5_SURFTrac.html</link>
      <guid isPermaLink="false">24f11ab1-cdc8-4ba2-b4ec-1e79b75ab6d5</guid>
      <pubDate>Sat, 6 Dec 2008 00:50:55 -0500</pubDate>
      <description>This is the result of my 3-month internship at Nokia Research Center, Palo Alto. We present a light-weight tracking method for outdoor augmented reality using pure computer vision technique with a database of geotagged and labeled images. &lt;br/&gt;&lt;br/&gt;After extracting and matching SURF features of the first video frame with the database, our method reliably tracks those features locally in the subsequent frames. This method is fast enough to be implemented on mobile devices. The technique is demonstrated in the context of augmenting a video stream with labels of buildings visible in the video.&lt;br/&gt;&lt;br/&gt;This project was demonstrated on N95 in ISMAR&#x2019;08 and CVPR&#x2019;09. Also, see an application of SURFTrac for 3D tracking and camera pose estimation &lt;a href="Entries/2009/8/15_SURFTrac3D.html"&gt;here&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Paper:&lt;br/&gt;&lt;br/&gt;D.N. Ta, W.C. Chen, N. Gelfand, and K. Pulli, &#x201C;SURFTrac: Efficient tracking and continuous object recognition using local feature descriptors,&#x201D; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. (&lt;a href="http://www.cc.gatech.edu/~dnth/Site/Publications_files/nguyen_cvpr09.pdf"&gt;pdf&lt;/a&gt;) &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;a href="http://www.youtube.com/watch?v=wRJTikAjRUo"&gt;Video&lt;/a&gt;:&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object036.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2008/12/5_SURFTrac.html#comment_layer"></iweb:comment>
    </item>
    <item>
      <title>Lego Edge Tracking</title>
      <link>http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2008/5/6_Lego_Edge_Tracking.html</link>
      <guid isPermaLink="false">75cc4187-8136-4993-ae59-623d7c664892</guid>
      <pubDate>Tue, 6 May 2008 15:08:17 -0400</pubDate>
      <description>This is a joint project with &lt;a href="http://www.cc.gatech.edu/~yjian6/"&gt;Yong-Dian Jian&lt;/a&gt; during my second year in Georgia Tech, to experiment with using Lego cube for Augmented Reality applications. We utilize the Lego&#x2019;s 3D model, which could be built easily, and develop a model-based edge tracking to track the 3D Lego cube in real-time. &lt;br/&gt;&lt;br/&gt;The Lego cube exposes interesting challenges for tracking algorithm. Its lack of texture fails most methods relying on point features. We use Canny method to extract edges from the capture image, however, the result is very unstable with many noisy edges from the background and shadow, and missing edges in the Lego model due to color similarity between some faces. Inspired by a paper of Georg Klein, we experimented with particle filter for edge tracking. The performance was improved but still unstable. The final stable result as you see in the video below was done with traditional Gauss-Newton non-linear least square optimization combine with many small tricks. :-)&lt;br/&gt;&lt;br/&gt;We would love to continue this project and make it more robust in the future.&lt;br/&gt;&lt;br/&gt;&lt;a href="http://www.youtube.com/watch?v=3nugeIPLppw"&gt;Video&lt;/a&gt;: &lt;br/&gt;&lt;br/&gt;</description>
      <iweb:image href="Media/object025.jpg"></iweb:image>
      <iweb:comment enabled="0" count="0" link="http://www.cc.gatech.edu/%7Ednth/Site/Duy-Nguyen/Entries/2008/5/6_Lego_Edge_Tracking.html#comment_layer"></iweb:comment>
    </item>
    <iweb:dateFormat>EEEE, MMMM d, y</iweb:dateFormat>
    <iweb:baseURL>http://www.cc.gatech.edu/~dnth/Site/Duy-Nguyen</iweb:baseURL>
    <iweb:maximumSummaryItems>12</iweb:maximumSummaryItems>
  </channel>
</rss>
